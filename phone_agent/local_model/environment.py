"""ç¯å¢ƒæ£€æµ‹æ¨¡å— - æ£€æµ‹ç³»ç»ŸCUDAã€æ˜¾å­˜ã€Pythonç‰ˆæœ¬ç­‰é…ç½®"""

import os
import sys
import platform
import subprocess
from dataclasses import dataclass
from typing import Optional, Tuple, List, Dict, Any

# Windowsä¸‹éšè—æ§åˆ¶å°çª—å£
_SUBPROCESS_FLAGS = {'creationflags': subprocess.CREATE_NO_WINDOW} if sys.platform == 'win32' else {}


@dataclass
class GPUInfo:
    """GPUä¿¡æ¯"""
    name: str
    memory_total: int  # MB
    memory_free: int = 0
    compute_capability: str = ""
    driver_version: str = ""


@dataclass
class GitInfo:
    """Gitç¯å¢ƒä¿¡æ¯"""
    git_available: bool
    git_version: Optional[str] = None
    lfs_available: bool = False
    lfs_version: Optional[str] = None


@dataclass
class SystemInfo:
    """ç³»ç»Ÿç¯å¢ƒä¿¡æ¯"""
    os_name: str
    os_version: str
    python_version: str
    cpu_cores: int
    ram_total: int  # MB
    cuda_available: bool
    cuda_version: Optional[str]
    gpus: List[GPUInfo]
    recommended_model: str
    recommended_quantization: str
    can_run_local: bool
    reason: str
    cudnn_version: Optional[str] = None
    git_info: Optional[GitInfo] = None


class EnvironmentDetector:
    """ç¯å¢ƒæ£€æµ‹å™¨ - ç®€æ´é«˜æ•ˆå®ç°"""
    
    def __init__(self):
        self.system_info: Optional[SystemInfo] = None
        
    def detect(self) -> SystemInfo:
        """ä¸€æ¬¡æ€§æ£€æµ‹æ‰€æœ‰ç³»ç»Ÿç¯å¢ƒä¿¡æ¯"""
        os_name, os_version = platform.system(), platform.version()
        python_ver = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
        
        cuda_available, cuda_version, gpus = self._detect_gpu_environment()
        ram_total = self._get_ram_total()
        git_info = self._detect_git_environment()
        
        model, quant, can_run, reason = self._get_recommendation(cuda_available, gpus, ram_total)
        
        self.system_info = SystemInfo(
            os_name=os_name, os_version=os_version, python_version=python_ver,
            cpu_cores=os.cpu_count() or 1, ram_total=ram_total,
            cuda_available=cuda_available, cuda_version=cuda_version, gpus=gpus,
            recommended_model=model, recommended_quantization=quant,
            can_run_local=can_run, reason=reason,
            git_info=git_info
        )
        return self.system_info
    
    def _detect_git_environment(self) -> GitInfo:
        """æ£€æµ‹Gitå’ŒGit LFSç¯å¢ƒ"""
        git_available = False
        git_version = None
        lfs_available = False
        lfs_version = None
        
        try:
            # æ£€æµ‹ git
            result = subprocess.run(
                ['git', '--version'],
                capture_output=True, text=True, timeout=5, **_SUBPROCESS_FLAGS
            )
            if result.returncode == 0:
                git_available = True
                # è§£æç‰ˆæœ¬å·ï¼Œä¾‹å¦‚ "git version 2.42.0.windows.1"
                version_text = result.stdout.strip()
                if 'version' in version_text:
                    git_version = version_text.split('version')[1].strip().split()[0]
                print(f"âœ… Git: {git_version}")
        except (FileNotFoundError, subprocess.TimeoutExpired, OSError):
            print("âš ï¸  Git æœªå®‰è£…")
        except Exception as e:
            print(f"âš ï¸  Git æ£€æµ‹å¼‚å¸¸: {e}")
        
        if git_available:
            try:
                # æ£€æµ‹ git-lfs
                result = subprocess.run(
                    ['git', 'lfs', 'version'],
                    capture_output=True, text=True, timeout=5, **_SUBPROCESS_FLAGS
                )
                if result.returncode == 0:
                    lfs_available = True
                    # è§£æç‰ˆæœ¬å·ï¼Œä¾‹å¦‚ "git-lfs/3.4.0 (GitHub; windows amd64; go 1.21.1)"
                    version_text = result.stdout.strip()
                    if '/' in version_text:
                        lfs_version = version_text.split('/')[1].split()[0]
                    print(f"âœ… Git LFS: {lfs_version}")
            except (FileNotFoundError, subprocess.TimeoutExpired, OSError):
                print("âš ï¸  Git LFS æœªå®‰è£…")
            except Exception as e:
                print(f"âš ï¸  Git LFS æ£€æµ‹å¼‚å¸¸: {e}")
        
        return GitInfo(
            git_available=git_available,
            git_version=git_version,
            lfs_available=lfs_available,
            lfs_version=lfs_version
        )
        
    def _get_ram_total(self) -> int:
        """è·å–ç³»ç»Ÿæ€»å†…å­˜(MB)"""
        if platform.system() == 'Windows':
            try:
                import ctypes
                kernel32 = ctypes.windll.kernel32
                class MEMSTAT(ctypes.Structure):
                    _fields_ = [('dwLength', ctypes.c_ulong), ('dwMemoryLoad', ctypes.c_ulong),
                               ('ullTotalPhys', ctypes.c_ulonglong), ('ullAvailPhys', ctypes.c_ulonglong),
                               ('ullTotalPageFile', ctypes.c_ulonglong), ('ullAvailPageFile', ctypes.c_ulonglong),
                               ('ullTotalVirtual', ctypes.c_ulonglong), ('ullAvailVirtual', ctypes.c_ulonglong),
                               ('ullAvailExtendedVirtual', ctypes.c_ulonglong)]
                stat = MEMSTAT()
                stat.dwLength = ctypes.sizeof(stat)
                kernel32.GlobalMemoryStatusEx(ctypes.byref(stat))
                return int(stat.ullTotalPhys / 1024 / 1024)
            except Exception:
                pass
        else:
            try:
                with open('/proc/meminfo') as f:
                    for line in f:
                        if 'MemTotal' in line:
                            return int(line.split()[1]) // 1024
            except Exception:
                pass
        return 8000
        
    def _detect_gpu_environment(self) -> Tuple[bool, Optional[str], List[GPUInfo]]:
        """ç»Ÿä¸€æ£€æµ‹GPUç¯å¢ƒ"""
        cuda_version = None
        gpus = []
        
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name,memory.total,memory.free,driver_version', 
                 '--format=csv,noheader,nounits'],
                capture_output=True, text=True, timeout=10, **_SUBPROCESS_FLAGS
            )
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        parts = [p.strip() for p in line.split(',')]
                        if len(parts) >= 4:
                            try:
                                gpus.append(GPUInfo(
                                    name=parts[0], memory_total=int(float(parts[1])),
                                    memory_free=int(float(parts[2])), driver_version=parts[3]
                                ))
                            except (ValueError, IndexError):
                                pass
                
                result2 = subprocess.run(['nvidia-smi'], capture_output=True, text=True, 
                                        timeout=10, **_SUBPROCESS_FLAGS)
                if result2.returncode == 0:
                    for line in result2.stdout.split('\n'):
                        if 'CUDA Version' in line:
                            try:
                                cuda_version = line.split('CUDA Version:')[1].split()[0].strip()
                            except (IndexError, ValueError):
                                pass
                            break
        except (FileNotFoundError, subprocess.TimeoutExpired, OSError) as e:
            print(f"âš ï¸  nvidia-smi æ£€æµ‹å¤±è´¥: {e}")
            pass
        except Exception as e:
            print(f"âš ï¸  GPU æ£€æµ‹å¼‚å¸¸: {e}")
            pass
            
        if not gpus:
            try:
                import torch
                if torch.cuda.is_available():
                    cuda_version = cuda_version or torch.version.cuda
                    for i in range(torch.cuda.device_count()):
                        props = torch.cuda.get_device_properties(i)
                        gpus.append(GPUInfo(
                            name=props.name, 
                            memory_total=props.total_memory // (1024 * 1024),
                            compute_capability=f"{props.major}.{props.minor}"
                        ))
            except ImportError:
                print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹CUDA")
                pass
            except Exception as e:
                print(f"âš ï¸  PyTorch GPU æ£€æµ‹å¼‚å¸¸: {e}")
                pass
                
        return bool(gpus), cuda_version, gpus
        
    def _get_recommendation(self, cuda_available: bool, gpus: List[GPUInfo], 
                            ram_total: int) -> Tuple[str, str, bool, str]:
        """æ ¹æ®ç¡¬ä»¶æ¨èæ¨¡å‹é…ç½®"""
        if not cuda_available or not gpus:
            return 'API_MODE', 'none', False, 'æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œå»ºè®®ä½¿ç”¨APIæ¨¡å¼'
            
        max_vram = max(gpu.memory_total for gpu in gpus)
        
        if max_vram >= 16000:
            return 'AutoGLM-Phone-9B', 'fp16', True, f'{max_vram}MBæ˜¾å­˜ï¼Œå¯è¿è¡ŒFP16æ¨¡å‹'
        elif max_vram >= 10000:
            return 'AutoGLM-Phone-9B', 'int8', True, f'{max_vram}MBæ˜¾å­˜ï¼Œæ¨èINT8é‡åŒ–'
        elif max_vram >= 6000:
            return 'AutoGLM-Phone-9B-GGUF-Q4', 'q4_k_m', True, f'{max_vram}MBæ˜¾å­˜ï¼Œæ¨èQ4é‡åŒ–'
        else:
            return 'API_MODE', 'none', False, f'æ˜¾å­˜ä¸è¶³({max_vram}MB)ï¼Œå»ºè®®ä½¿ç”¨APIæ¨¡å¼'
            
    def get_torch_install_command(self) -> str:
        """è·å–PyTorchå®‰è£…å‘½ä»¤"""
        if not self.system_info:
            self.detect()
        cuda_ver = self.system_info.cuda_version or ''
        
        if not self.system_info.cuda_available:
            return 'pip install torch torchvision torchaudio'
        if cuda_ver.startswith('12'):
            return 'pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121'
        elif cuda_ver.startswith('11'):
            return 'pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118'
        return 'pip install torch torchvision torchaudio'
            
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        if not self.system_info:
            self.detect()
        info = self.system_info
        return {
            'os': f"{info.os_name} {info.os_version}", 'python': info.python_version,
            'cpu_cores': info.cpu_cores, 'ram_gb': round(info.ram_total / 1024, 1),
            'cuda_available': info.cuda_available, 'cuda_version': info.cuda_version,
            'gpus': [{'name': g.name, 'vram_mb': g.memory_total} for g in info.gpus],
            'recommended': info.recommended_model, 'can_run_local': info.can_run_local,
            'reason': info.reason
        }
        
    def print_summary(self):
        """æ‰“å°ç¯å¢ƒæ‘˜è¦"""
        if not self.system_info:
            self.detect()
        info = self.system_info
        
        print("\n" + "=" * 50)
        print("ğŸ“Š ç³»ç»Ÿç¯å¢ƒæ£€æµ‹")
        print("=" * 50)
        print(f"ğŸ–¥ï¸  ç³»ç»Ÿ: {info.os_name} | Python: {info.python_version}")
        print(f"ğŸ’¾ å†…å­˜: {info.ram_total / 1024:.1f} GB | CPU: {info.cpu_cores}æ ¸")
        print(f"ğŸ® CUDA: {'âœ… ' + (info.cuda_version or 'å¯ç”¨') if info.cuda_available else 'âŒ ä¸å¯ç”¨'}")
        
        for gpu in info.gpus:
            print(f"ğŸ–¼ï¸  {gpu.name} ({gpu.memory_total / 1024:.1f}GB)")
            
        print(f"\nğŸ’¡ æ¨è: {info.recommended_model} ({info.recommended_quantization})")
        print(f"   {'âœ…' if info.can_run_local else 'âŒ'} {info.reason}")
        print("=" * 50 + "\n")


if __name__ == '__main__':
    detector = EnvironmentDetector()
    detector.detect()
    detector.print_summary()
