"""æœ¬åœ°æ¨¡å‹ç®¡ç†å™¨ - æ•´åˆç¯å¢ƒæ£€æµ‹ã€æ¨¡å‹ä¸‹è½½å’ŒæœåŠ¡ç®¡ç†"""

import sys
import json
import subprocess
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, Callable, Dict, Any, List

from phone_agent.local_model.environment import EnvironmentDetector, SystemInfo
from phone_agent.local_model.downloader import ModelDownloader, DownloadProgress, VLLMServerManager


@dataclass
class LocalModelConfig:
    """æœ¬åœ°æ¨¡å‹é…ç½®"""
    model_name: str
    model_path: str
    quantization: str = "fp16"
    port: int = 8000
    gpu_memory_utilization: float = 0.9
    max_model_len: int = 8192


class LocalModelManager:
    """æœ¬åœ°æ¨¡å‹ç®¡ç†å™¨ - ç®€æ´é«˜æ•ˆå®ç°"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.base_dir = Path.home() / '.autoglm'
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        self.config_path = Path(config_path) if config_path else self.base_dir / 'config.json'
        self.environment = EnvironmentDetector()
        self.downloader = ModelDownloader(str(self.base_dir / 'models'))
        self.server: Optional[VLLMServerManager] = None
        
        self._config = self._load_config()
        self._status_callback: Optional[Callable[[str, Dict[str, Any]], None]] = None
        
    def _load_config(self) -> Dict[str, Any]:
        if self.config_path.exists():
            try:
                return json.loads(self.config_path.read_text(encoding='utf-8'))
            except Exception:
                pass
        return {}
        
    def _save_config(self):
        try:
            self.config_path.write_text(json.dumps(self._config, indent=2, ensure_ascii=False), encoding='utf-8')
        except Exception:
            pass
            
    def set_status_callback(self, callback: Callable[[str, Dict[str, Any]], None]):
        self._status_callback = callback
        
    def _notify_status(self, event: str, data: Dict[str, Any] = None):
        if self._status_callback:
            try:
                self._status_callback(event, data or {})
            except Exception:
                pass
                
    def check_environment(self) -> SystemInfo:
        self._notify_status('environment_check_start')
        info = self.environment.detect()
        self._notify_status('environment_check_done', self.environment.to_dict())
        return info
        
    def get_recommended_setup(self) -> Dict[str, Any]:
        """è·å–æ¨èé…ç½®æ–¹æ¡ˆ"""
        if not self.environment.system_info:
            self.check_environment()
            
        info = self.environment.system_info
        
        if not info.can_run_local:
            return {
                'can_run_local': False, 'reason': info.reason,
                'recommended_model': 'API_MODE', 'recommended_quantization': 'none',
                'steps': [{'step': 1, 'description': 'ä½¿ç”¨äº‘ç«¯APIæ¨¡å¼', 'action': 'use_api'},
                         {'step': 2, 'description': 'é…ç½®APIå¯†é’¥', 'action': 'config_api_key'}]
            }
            
        # æ„å»ºå®‰è£…æ­¥éª¤
        steps, step_num = [], 1
        
        # æ£€æŸ¥ä¾èµ–å¹¶æ·»åŠ å®‰è£…æ­¥éª¤
        dep_checks = [
            ('torch', lambda: __import__('torch').cuda.is_available(), 
             'install_pytorch', self.environment.get_torch_install_command()),
            ('vllm', lambda: __import__('vllm'), 'install_vllm', 'pip install vllm'),
            ('transformers', lambda: __import__('transformers'), 'install_transformers', 'pip install transformers'),
        ]
        
        for name, check_fn, action, cmd in dep_checks:
            try:
                check_fn()
            except (ImportError, RuntimeError):
                steps.append({'step': step_num, 'description': f'å®‰è£…{name}', 'action': action, 'command': cmd})
                step_num += 1
                
        # ä¸‹è½½æ¨¡å‹
        if not self.downloader.is_model_downloaded(info.recommended_model):
            steps.append({'step': step_num, 'description': f'ä¸‹è½½æ¨¡å‹: {info.recommended_model}', 
                         'action': 'download_model', 'model': info.recommended_model})
            step_num += 1
            
        steps.append({'step': step_num, 'description': 'å¯åŠ¨æœ¬åœ°æ¨ç†æœåŠ¡', 'action': 'start_server'})
        
        return {
            'can_run_local': True, 'reason': info.reason,
            'recommended_model': info.recommended_model, 'recommended_quantization': info.recommended_quantization,
            'steps': steps
        }
        
    def _pip_install(self, packages: List[str]) -> bool:
        """å®‰è£…pipåŒ…"""
        for pkg in packages:
            try:
                __import__(pkg.split('[')[0])  # å¤„ç† pkg[extras] æ ¼å¼
            except ImportError:
                proc = subprocess.Popen(
                    [sys.executable, '-m', 'pip', 'install', pkg, '-q'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
                )
                proc.wait(timeout=120)
                if proc.returncode != 0:
                    return False
        return True
        
    def install_dependencies(self, progress_callback: Optional[Callable[[str, float], None]] = None) -> bool:
        """å®‰è£…å¿…è¦çš„ä¾èµ–"""
        try:
            if progress_callback:
                progress_callback('æ£€æŸ¥ç¯å¢ƒ...', 0.05)
                
            if not self.environment.system_info:
                self.check_environment()
            
            print("ğŸ“¦ å¼€å§‹å®‰è£…ä¾èµ–...")
                
            # å®‰è£…PyTorch
            if progress_callback:
                progress_callback('å®‰è£…PyTorch...', 0.1)
            
            torch_cmd = self.environment.get_torch_install_command()
            print(f"ğŸ“ æ‰§è¡Œ: {torch_cmd}")
            proc = subprocess.Popen(
                torch_cmd.split(),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
            )
            stdout, stderr = proc.communicate(timeout=120)
            if proc.returncode != 0:
                error_msg = f"å®‰è£…PyTorchå¤±è´¥: {stderr}"
                print(f"âŒ {error_msg}")
                raise Exception(error_msg)
            print("âœ… PyTorch å®‰è£…æˆåŠŸ")
                
            # å®‰è£…å…¶ä»–ä¾èµ–
            if progress_callback:
                progress_callback('å®‰è£…vLLMå’Œå…¶ä»–ä¾èµ–...', 0.5)
            
            deps = ['vllm', 'transformers', 'accelerate', 'sentencepiece', 'requests']
            for dep in deps:
                print(f"ğŸ“¦ å®‰è£… {dep}...")
                proc = subprocess.Popen(
                    [sys.executable, '-m', 'pip', 'install', dep, '-q'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
                )
                stdout, stderr = proc.communicate(timeout=60)
                if proc.returncode != 0:
                    error_msg = f"å®‰è£…{dep}å¤±è´¥: {stderr}"
                    print(f"âš ï¸  {error_msg}")
                    # ç»§ç»­å°è¯•å®‰è£…å…¶ä»–ä¾èµ–
                else:
                    print(f"âœ… {dep} å®‰è£…æˆåŠŸ")
                
            if progress_callback:
                progress_callback('å®‰è£…å®Œæˆ', 1.0)
            print("âœ… æ‰€æœ‰ä¾èµ–å®‰è£…å®Œæˆ")
            return True
            
        except Exception as e:
            error_msg = f'å®‰è£…ä¾èµ–å¤±è´¥: {str(e)}'
            self._notify_status('install_error', {'error': error_msg})
            print(f"âŒ {error_msg}")
            return False
            
    def download_model(self, model_name: str = None,
                       progress_callback: Optional[Callable[[DownloadProgress], None]] = None,
                       use_mirror: bool = True) -> bool:
        """ä¸‹è½½æ¨¡å‹"""
        if not model_name:
            if not self.environment.system_info:
                self.check_environment()
            model_name = self.environment.system_info.recommended_model
            
        if model_name == 'API_MODE':
            return False
            
        self._notify_status('download_start', {'model': model_name})
        success = self.downloader.download_model(model_name, progress_callback, use_mirror)
        
        if success:
            self._config.update({'last_model': model_name, 
                               'model_path': str(self.downloader.get_model_path(model_name))})
            self._save_config()
            self._notify_status('download_complete', {'model': model_name})
        else:
            self._notify_status('download_error', {'model': model_name, 
                               'error': self.downloader.progress.error_message})
        return success
        
    def start_server(self, model_name: str = None, port: int = 8000,
                     gpu_memory_utilization: float = 0.9) -> bool:
        """å¯åŠ¨æœ¬åœ°æ¨ç†æœåŠ¡"""
        try:
            model_name = model_name or self._config.get('last_model')
            if not model_name:
                error_msg = 'æœªæŒ‡å®šæ¨¡å‹'
                self._notify_status('server_error', {'error': error_msg})
                print(f"âŒ {error_msg}")
                return False
                
            model_path = str(self.downloader.get_model_path(model_name))
            if not self.downloader.is_model_downloaded(model_name):
                error_msg = f'æ¨¡å‹æœªä¸‹è½½: {model_name}'
                self._notify_status('server_error', {'error': error_msg})
                print(f"âŒ {error_msg}")
                return False
            
            print(f"ğŸ“ å³å°†å¯åŠ¨æœåŠ¡: æ¨¡å‹={model_name}, ç«¯å£={port}, æ¨¡å‹è·¯å¾„={model_path}")
            self._notify_status('server_starting', {'model': model_name, 'port': port})
            
            self.server = VLLMServerManager(model_path, port)
            
            if self.server.start(gpu_memory_utilization=gpu_memory_utilization):
                self._config['server_port'] = port
                self._config['last_model'] = model_name
                self._save_config()
                api_base = self.server.get_api_base()
                self._notify_status('server_started', {'model': model_name, 'port': port, 'api_base': api_base})
                print(f"âœ… æœåŠ¡å·²å¯åŠ¨: {api_base}")
                return True
            else:
                error_msg = 'æœåŠ¡å¯åŠ¨å¤±è´¥'
                self._notify_status('server_error', {'error': error_msg})
                print(f"âŒ {error_msg}")
                return False
                
        except Exception as e:
            error_msg = f'å¯åŠ¨æœåŠ¡å¼‚å¸¸: {str(e)}'
            self._notify_status('server_error', {'error': error_msg})
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return False
        
    def stop_server(self):
        """åœæ­¢æ¨ç†æœåŠ¡"""
        try:
            if self.server:
                self.server.stop()
            self._notify_status('server_stopped')
        except Exception as e:
            print(f"âš ï¸  åœæ­¢æœåŠ¡æ—¶å‡ºé”™: {e}")
            
    def is_server_running(self) -> bool:
        return self.server is not None and self.server.is_running()
        
    def get_api_base(self) -> Optional[str]:
        return self.server.get_api_base() if self.is_server_running() else None
        
    def get_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€"""
        if not self.environment.system_info:
            self.check_environment()
        return {
            'environment': self.environment.to_dict(),
            'models_downloaded': self.downloader.get_downloaded_models(),
            'server_running': self.is_server_running(),
            'api_base': self.get_api_base(),
            'config': self._config
        }
        
    def auto_setup(self, progress_callback: Optional[Callable[[str, float, str], None]] = None) -> bool:
        """è‡ªåŠ¨è®¾ç½®ï¼šæ£€æµ‹ç¯å¢ƒã€å®‰è£…ä¾èµ–ã€ä¸‹è½½æ¨¡å‹ã€å¯åŠ¨æœåŠ¡"""
        try:
            # 1. æ£€æµ‹ç¯å¢ƒ
            if progress_callback:
                progress_callback('æ£€æµ‹ç¯å¢ƒ', 0.05, 'æ­£åœ¨æ£€æµ‹ç³»ç»Ÿé…ç½®...')
            self.check_environment()
            
            if not self.environment.system_info.can_run_local:
                if progress_callback:
                    progress_callback('æ£€æµ‹å®Œæˆ', 1.0, self.environment.system_info.reason)
                return False
                
            # 2. å®‰è£…ä¾èµ–
            if progress_callback:
                progress_callback('å®‰è£…ä¾èµ–', 0.1, 'æ­£åœ¨å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…...')
            if not self.install_dependencies(lambda m, p: progress_callback('å®‰è£…ä¾èµ–', 0.1 + p * 0.2, m) if progress_callback else None):
                return False
                
            # 3. ä¸‹è½½æ¨¡å‹
            model_name = self.environment.system_info.recommended_model
            if not self.downloader.is_model_downloaded(model_name):
                if progress_callback:
                    progress_callback('ä¸‹è½½æ¨¡å‹', 0.3, f'æ­£åœ¨ä¸‹è½½ {model_name}...')
                if not self.download_model(model_name, 
                    lambda dp: progress_callback('ä¸‹è½½æ¨¡å‹', 0.3 + dp.total_percent / 100 * 0.5, 
                               f'{dp.current_file} ({dp.percent:.1f}%)') if progress_callback else None):
                    return False
            elif progress_callback:
                progress_callback('ä¸‹è½½æ¨¡å‹', 0.8, 'æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½')
                
            # 4. å¯åŠ¨æœåŠ¡
            if progress_callback:
                progress_callback('å¯åŠ¨æœåŠ¡', 0.85, 'æ­£åœ¨å¯åŠ¨æ¨ç†æœåŠ¡...')
            if not self.start_server(model_name):
                return False
                
            if progress_callback:
                progress_callback('å®Œæˆ', 1.0, f'æœ¬åœ°æœåŠ¡å·²å¯åŠ¨: {self.get_api_base()}')
            return True
            
        except Exception as e:
            if progress_callback:
                progress_callback('é”™è¯¯', 1.0, str(e))
            return False


def quick_setup() -> LocalModelManager:
    """å¿«é€Ÿè®¾ç½®æœ¬åœ°æ¨¡å‹"""
    manager = LocalModelManager()
    print("ğŸ” æ£€æµ‹ç³»ç»Ÿç¯å¢ƒ...")
    manager.check_environment()
    manager.environment.print_summary()
    
    if not manager.environment.system_info.can_run_local:
        print("âŒ å½“å‰ç¯å¢ƒä¸æ”¯æŒæœ¬åœ°è¿è¡Œï¼Œè¯·ä½¿ç”¨APIæ¨¡å¼")
        return manager
        
    print("\nğŸ“‹ æ¨èè®¾ç½®æ­¥éª¤:")
    for step in manager.get_recommended_setup()['steps']:
        print(f"  {step['step']}. {step['description']}")
    return manager


if __name__ == '__main__':
    quick_setup()
